<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1000px;
	}

	h1 {
		font-weight:300;
		font-size:24px;
	}

	code {
    font-size: 0.8rem;
    margin: 0 0.1rem;
    padding: 0.1rem 0.1rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
}

pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
    text-align: left;
}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	p.small {
		font-size: 12px
	}
</style>

<html>
  <head>
		<title>Disentangling visual and written concepts in CLIP</title>
<!-- 		<meta property="og:image" content="http://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/images/teaser.png"/>
		<meta property="og:title" content="Stereo Magnification: Learning View Synthesis using Multiplane Images" /> -->
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:36px">Disentangling visual and written concepts in CLIP</span>
	</center>
    
	<br>
  	<table align=center width=700px>
  	 <tr>
		<td align=center width=100px>
		<center>
		<span style="font-size:20px"><a href='https://joaanna.github.io/'>Joanna Materzynska</a></span>
		</center>
		</td>

		<td align=center width=100px>
		<center>
		<span style="font-size:20px"><a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a></span>
		</center>
		</td>

		<td align=center width=100px>
		<center>
		<span style="font-size:20px"><a href="https://baulab.info/">David Bau</a></span>
		</center>
		</td>

	 </tr>
	</table>

	<table align=center width=700px>
  	 <tr>
		<td align=center width=100px>
		<center>
		<span style="font-size:18px">MIT CSAIL</span></center>
		</center>
		</td>
	 </tr>
	</table>

		<br>
	<table align=center width=700px>
  	 <tr>
		<td align=center width=100px>
		<center>
		<span style="font-size:18px">CVPR 2022 (ORAL)</span></center>
		</center>
		</td>
	 </tr>
	</table>

	<table align=center width=500px style="margin-top:30px">
  	 <tr>
		<td align=center width=50px>
		<center>
		<span style="font-size:24px"><a href="https://arxiv.org/abs/ADD">[Paper]</a></span>
		</center>
		</td>

		<td align=center width=20px>
		<center>
		<span style="font-size:24px"><a href="https://github.com/joaanna/disentangling_spelling_in_clip">[Code]</a></span>
		</center>
		</td>

<!--		<td align=center width=20px>-->
<!--		<center>-->
<!--		<span style="font-size:24px"><a href="#video">[Video]</a></span>-->
<!--		</center>-->
<!--		</td>-->

<!--		<td align=center width=20px>-->
<!--		<center>-->
<!--		<span style="font-size:24px"><a href="https://huggingface.co/spaces/carolineec/informativedrawings">[Demo]</a></span>-->
<!--		</center>-->
<!--		</td>-->

		 <td align=center width=20px>
		<center>
		<span style="font-size:24px"><a href="https://huggingface.co/spaces/carolineec/informativedrawings">[Poster]</a></span>
		</center>
		</td>

		 <td align=center width=20px>
		<center>
		<span style="font-size:24px"><a href="https://drive.google.com/drive/folders/17v74pmQs2zHrs9sJ_8D0bfrz5ykB5ZW2?usp=sharing">[Dataset]</a></span>
		</center>
		</td>

	 </tr>
	</table>
 
  		  <br>
  		  <table align=center width=900px>
  			  <tr>
  	              <td width=600px>
  					<center>
  	                	<a href="./images/teaser.png"><img src = "images/teaser.png" width="1000px"></img></href></a><br>
					</center>
  	              </td>
  	          </tr>
  		  </table>

  		  <table align=center width=1000>
	 		<center><h1 style="margin-top:30px">Abstract</h1></center>
  			  <br>
				  <td>
				  	<br>
			      	  <p style="text-align:justify;margin-top:-30px">
						The CLIP network measures the similarity between natural text and images; in this work, we investigate the entanglement of the representation of word images and natural images in its image encoder. First, we find that the image encoder has an ability to match word images with natural images of scenes described by those words. This is consistent with previous research that suggests that the meaning and the spelling of a word might be entangled deep within the network. On the other hand, we also find that CLIP has a strong ability to match nonsense words, suggesting that processing of letters is separated from processing of their meaning. To explicitly determine whether the spelling capability of CLIP is separable, we devise a procedure for identifying representation subspaces that selectively isolate or eliminate spelling capabilities. We benchmark our methods against a range of retrieval tasks, and we also test them by measuring the appearance of text in CLIP-guided generated images. We find that our methods are able to cleanly separate spelling capabilities of CLIP from the visual processing of natural images.
			      	  </p>
				  </td>
			  </br>
  	              </td>
              </tr>
  		  </table>

<hr>
		 <!-- <table align=center width=550px> -->
  		  <table align=center width=1000 style="text-align: justify;">
	 		<center><h1>Method</h1></center>
  			  <tr>
  			  	<td>
  			  		<img style="height:300px; margin-right: 20px;" src="images/method.png"/>
  			  	<td>
				We identify an orthogonal, lower-dimensional projection of the learned representations to disentangle the CLIP vector spaceâ€™s visual space from the written one. To this end, we collect a dataset consisting of tuples with five elements natural images and their text class labels, image texts and text strings, and the natural image with the string from the synthetic image text rendered on it. Different pairs are trained to minimize their distance in the projection space. The losses in red correspond to the task of visual concepts, and the losses in blue to the distilling written words. We show the effectivness of the projection on various tasks from text-to-image generation to OCR.

  	          </td>
              </tr>
  		  </table>
	<br>
<hr>

	  		  <table align=center width=1000>
	 		<center><h1 style="margin-top:30px">Text-to-image generation with learned projections</h1></center>
  			  <tr>
				  <td>
				  	<br>
			      	  <p style="text-align:justify;margin-top:-30px">
						As a canvas for paining our learned projections, we use the VQGAN-CLIP framework to generate images besed on a text prompt. On one hand, we consider the CLIP network, the Learn to Spell model and the Forget to Spell model.
			      	  </p>
  	              </td>

              </tr>
				  <tr>
					  				  <td>
					  <embed width="1000px" src="images/big_fig_horizontal.png"/>
				  </td>
				  </tr>
  		  </table>

<hr>

		  		  <table align=center width=1000>
	 		<center><h1 style="margin-top:30px">Evaluation</h1></center>
  			  <tr>
  			  	<td>
  			  		<img style="height:300px; margin-right: 20px;" src="images/annotated_textdetection.png"/>
  			  	<td>
				    			  	<td>
  			  		<img style="height:300px; margin-right: 20px;" src="images/generation_eval_2k_per_set.png"/>
  			  	<td>
  	          </td>
              </tr>
			 <center> <tr>
				  <td><br>
					  <p>
				  We measure the appearance of text in images using an off the shelf OCR detector.
					  </p>
				  </td>

			  </tr></center>
  		  </table>

<hr>

			  		  <table align=center width=1000>
	 		<center><h1 style="margin-top:30px">Typographic attack</h1></center>
  			  <tr>
  			  	<td>
  			  		<img style="height:400px; margin-right: 20px;" src="images/typo_cameraready.png"/>
  	          </td>
              </tr>
			 <center> <tr>
				  <td><br>
					  <p>
				   A test on a data set of 200 text attack images, a) shows a similarity matrix between the embeddings images with typographic attacks and the the text embeddings of typographic attack labels and true object labels obtained by the CLIP model, b) shows the same similarity matrix obtained by the Forget-to-Spell model.
					  </p>
				  </td>

			  </tr></center>
  		  </table>

<hr>

	  	<center><h1>Bibtex</h1>

			<table align=center width=800px>
				<tr>
				<td>
		      <pre><code>
	      @inproceedings{materzynska2022disentangling,
	      title={Disentangling visual and written concepts in CLIP},
	      author={Materzynska, Joanna and Torralba, Antonio and Bau, David},
	      booktitle={CVPR},
	      year={2022}
	      }
      </code></pre>
					</td>

				</tr>
			</table>
			</center>
	  	


<hr>
  		  <table align=center width=1000px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>

				We are grateful to Manel Baradad for early feedback and valuable discussions. JM was partially funded by the MIT- IBM Watson AI Lab, and DB was supported by DARPA SAIL-ON HR0011-20-C-0022. This webpage template was recycled from <a href="https://richzhang.github.io/colorization/">here</a>.

			</left>
		</td>
		</tr>
		</table>
		<br><br>


</body>
</html>
